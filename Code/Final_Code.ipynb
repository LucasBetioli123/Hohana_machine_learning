{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csy-HsXqe289"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m14t1MPanR6j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "import random\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, PReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from scipy.ndimage import binary_dilation\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYuF7t5we-u9"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpHrwg92CGtE"
      },
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(images, image_type, view):\n",
        "    \"\"\"\n",
        "    Prepare data.\n",
        "\n",
        "    Args:\n",
        "        images (np.ndarray): The peaks image extracted from DWI images or binary masks resulted from the segmentation tracts.\n",
        "        image_type (int): if it's DWI or mask data.\n",
        "        view (int): choose between coronal, axial or saggital.\n",
        "    Returns:\n",
        "        The images sliced.\n",
        "    \"\"\"\n",
        "    images_sliced = []\n",
        "    for f in images:\n",
        "        if image_type == 'peaks':\n",
        "            a = nib.load(f).get_fdata()[:144, 15:159, :144, :9]\n",
        "            a = np.nan_to_num(a)\n",
        "        else:\n",
        "            a = nib.load(f).get_fdata()[:144, 15:159, :144]\n",
        "            a = np.nan_to_num(a)\n",
        "            a = np.expand_dims(a, -1)\n",
        "\n",
        "        for i in range(144):\n",
        "            # axial view\n",
        "            if view == 'axial':\n",
        "                slices = a[:, :, i, :]\n",
        "            # coronal view\n",
        "            elif view == 'coronal':\n",
        "                slices = a[:, i, :, :]\n",
        "            # sagittal view\n",
        "            elif view == 'sagittal':\n",
        "                slices = a[i, :, :, :]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown view type: {view}\")\n",
        "            images_sliced.append(slices)\n",
        "    return images_sliced\n",
        "\n",
        "def generate_dataset(data, peaks_sliced, mask_sliced, batch_size):\n",
        "    \"\"\"\n",
        "    Generate dataset.\n",
        "\n",
        "    Args:\n",
        "        data (int): prepare data for train or test.\n",
        "        peaks_sliced (np.ndarray): Sliced DWI images.\n",
        "        mask_sliced (np.ndarray): Sliced binary masks images.\n",
        "        batch_size (int): size of batch.\n",
        "    Returns:\n",
        "        Dataset.\n",
        "    \"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((peaks_sliced, mask_sliced)) if data == 'train' else tf.data.Dataset.from_tensor_slices((peaks_sliced))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Ur7ak6dtqFPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl7bGwzQCYn2"
      },
      "source": [
        "### Model UNET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_shape=(144, 144, 9)):\n",
        "    # Set image data format to channels last\n",
        "    keras.backend.set_image_data_format(\"channels_last\")\n",
        "\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Contracting path\n",
        "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.4)(pool4)\n",
        "\n",
        "    # Bottom layer\n",
        "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "\n",
        "    # Expansive path\n",
        "    up6 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "    up6 = concatenate([up6, conv4], axis=3)\n",
        "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "    up7 = concatenate([up7, conv3], axis=3)\n",
        "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
        "    up8 = concatenate([up8, conv2], axis=3)\n",
        "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
        "    up9 = concatenate([up9, conv1], axis=3)\n",
        "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "    return model\n",
        "\n",
        "unet = unet_model()\n",
        "unet.summary()"
      ],
      "metadata": {
        "id": "5WWkopI7g5Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qDTfYaCfVw"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    \"\"\"\n",
        "    Dice coeficient calc.\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): Ground truth images.\n",
        "        y_pred (np.ndarray): Prediction images.\n",
        "    Returns:\n",
        "        Dice score.\n",
        "    \"\"\"\n",
        "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    dice = tf.reduce_mean((2. * intersection + smooth) / (union + smooth))\n",
        "    return dice\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "Ww_pLz5yimSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_unet(peaks_images, mask_images, val_peaks_images, val_masks_images, save_dir, batch_size=56, epochs=130, learning_rate=0.002, shuffle=False, verbose=1):\n",
        "        \"\"\"\n",
        "    Train a U-Net model.\n",
        "\n",
        "    Args:\n",
        "        peaks_images (np.ndarray): The peaks image extracted from DWI images.\n",
        "        mask_images (np.ndarray): The binary masks resulted from the segmentation tracts.\n",
        "        val_peaks_images (np.ndarray): The validation peaks image.\n",
        "        val_masks_images (np.ndarray): The validation binary masks.\n",
        "        save_dir (str): The directory where the best model weights will be saved.\n",
        "        epochs (int): The number of epochs to train the model for each fold.\n",
        "        learning_rate (float): The learning rate for the Adamax optimizer.\n",
        "        shuffle (str): if set True it will shuffle the slices from the dataset during the training.\n",
        "        verbose (int): Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "\n",
        "    Returns:\n",
        "        The history object of the trained model.\n",
        "        \"\"\"\n",
        "        # Prepare the training and validation datasets\n",
        "        train_dataset = generate_dataset('train', peaks_images, mask_images, batch_size=batch_size)\n",
        "        val_dataset = generate_dataset('train', val_peaks_images, val_masks_images, batch_size=batch_size)\n",
        "\n",
        "        # Define the U-Net model\n",
        "        unet = unet_model()\n",
        "\n",
        "        # Define the checkpoint callback to save the best model weights\n",
        "        checkpoint_callback = ModelCheckpoint(filepath=save_dir,\n",
        "                                            save_best_only=False,\n",
        "                                            save_weights_only=False,\n",
        "                                            monitor='val_loss',\n",
        "                                            mode='min',\n",
        "                                            verbose=verbose)\n",
        "\n",
        "        # Compile the model\n",
        "        unet.compile(optimizer=Adamax(learning_rate=learning_rate),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=[dice_coef])\n",
        "\n",
        "        # Train the model\n",
        "        history = unet.fit(train_dataset,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=val_dataset,\n",
        "                        callbacks=[checkpoint_callback],\n",
        "                        shuffle = shuffle,\n",
        "                        verbose=verbose)\n",
        "        return history"
      ],
      "metadata": {
        "id": "Y-J5W-ZEiNLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZkaqyICo-m"
      },
      "source": [
        "### Prediction in new data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load models, predict and save"
      ],
      "metadata": {
        "id": "Pvd1mH-DOT6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_data(peaks_images, path_model, view, num_subjects):\n",
        "    \"\"\"\n",
        "    Predict new data using a trained model.\n",
        "\n",
        "    Args:\n",
        "        peaks (list): List of paths to the peaks images.\n",
        "        path_model (str): Path to the trained model.\n",
        "        view (str): The view for which to perform the prediction. Must be one of 'coronal', 'axial', or 'sagital'.\n",
        "        num_subjects (int): Number of subjects in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        List of predicted data for each subject.\n",
        "    \"\"\"\n",
        "    model = keras.models.load_model(path_model, custom_objects={'dice_coef': dice_coef})\n",
        "\n",
        "    peaks_sliced = prepare_data(peaks_images, 'peaks', view)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(peaks_sliced).batch(1)\n",
        "    predictions = model.predict(dataset)\n",
        "\n",
        "    if view == 'coronal':\n",
        "        pred = np.transpose(predictions, (1, 0, 2, 3))\n",
        "        axis = 1\n",
        "    elif view == 'axial':\n",
        "        pred = np.transpose(predictions, (1, 2, 0, 3))\n",
        "        axis = 2\n",
        "    else:  # 'sagital'\n",
        "        pred = np.transpose(predictions, (0, 1, 2, 3))\n",
        "        axis = 0\n",
        "\n",
        "    pred = np.split(pred, num_subjects, axis=axis)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "xxHFj-slNFS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_data(data):\n",
        "  \"\"\"\n",
        "      Resize the prediction data (144x144x144)\n",
        "      to the original size(145x175x145).\n",
        "  \"\"\"\n",
        "    data_resized = []\n",
        "    for n in data:\n",
        "        mat = np.zeros((144, 15, 144), dtype=n.dtype)\n",
        "        data_res = np.concatenate([mat, n, mat], axis=1)\n",
        "        data_res = np.insert(data_res, 144, 0, axis=0)\n",
        "        data_res = np.insert(data_res, 144, 0, axis=2)\n",
        "        data_resized.append(data_res)\n",
        "    return data_resized"
      ],
      "metadata": {
        "id": "1XJlPEdy5XFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions(coronal_predictions, axial_predictions, sagittal_predictions, flip=True, mean=False, reference_files=None, save_path='.', format='nii', threshold_factor=0.5):\n",
        "    \"\"\"\n",
        "    Predicts masks for new data using a trained model and saves the results.\n",
        "\n",
        "    Args:\n",
        "        coronal_predictions (np.ndarray): Input images results from coronal model.\n",
        "        axial_predictions (np.ndarray): Input images results from axial model.\n",
        "        sagital_predictions (np.ndarray): Input images results from sagital model.\n",
        "        flip (bool, optional): Whether to flip the predictions along the second axis. Defaults to True.\n",
        "        mean (bool, optional): Whether to average the predictions along the fifth axis. Defaults to False.\n",
        "        ids (list, optional): List of ids for each input image. Defaults to None.\n",
        "        reference_files (list, optional): List of reference files for each input image. Defaults to None.\n",
        "        save_path (str, optional): Path to save the predicted masks. Defaults to '.'.\n",
        "        format (str, optional): Format to save the predicted masks ('nii' or 'npy'). Defaults to 'nii'.\n",
        "        threshold_factor (float, optional): Factor to multiply the maximum prediction value to determine the threshold for creating the binary mask. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        list: List of predicted masks.\n",
        "    \"\"\"\n",
        "\n",
        "    if flip:\n",
        "        coronal_predictions = np.flip(coronal_predictions, axis=1)\n",
        "        axial_predictions = np.flip(axial_predictions, axis=1)\n",
        "        sagittal_predictions = np.flip(sagittal_predictions, axis=1)\n",
        "\n",
        "    result = np.concatenate((coronal_predictions, axial_predictions, sagittal_predictions), axis=4)\n",
        "\n",
        "    if mean:\n",
        "        result = np.mean(result, axis=4)\n",
        "        result = resize_data(result)\n",
        "\n",
        "    # Create masks from predictions\n",
        "    ids = ['pred_01', 'pred_02', 'pred_03', 'pred_04', 'pred_05', 'pred_06', 'pred_07', 'pred_08', 'pred_09', 'pred_10', 'pred_11', 'pred_12', 'pred_13', 'pred_14', 'pred_15', 'pred_16', 'pred_17', 'pred_18', 'pred_19', 'pred_20','pred_21', 'pred_22', 'pred_23', 'pred_24', 'pred_25', 'pred_26', 'pred_27', 'pred_28', 'pred_29', 'pred_30','pred_31', 'pred_32', 'pred_33', 'pred_34', 'pred_35', 'pred_36', 'pred_37', 'pred_38', 'pred_39', 'pred_40','pred_41', 'pred_42', 'pred_43', 'pred_44', 'pred_45', 'pred_46', 'pred_47', 'pred_48', 'pred_49', 'pred_50','pred_51', 'pred_52', 'pred_53', 'pred_54', 'pred_55', 'pred_56', 'pred_57', 'pred_58', 'pred_59', 'pred_60','pred_61', 'pred_62', 'pred_63', 'pred_64', 'pred_65', 'pred_66', 'pred_67', 'pred_68', 'pred_69', 'pred_70','pred_71', 'pred_72', 'pred_73', 'pred_74', 'pred_75', 'pred_76', 'pred_77', 'pred_78', 'pred_79', 'pred_80','pred_81', 'pred_82', 'pred_83', 'pred_84', 'pred_85', 'pred_86', 'pred_87', 'pred_88', 'pred_89', 'pred_90','pred_91', 'pred_92', 'pred_93', 'pred_94', 'pred_95', 'pred_96', 'pred_97', 'pred_98', 'pred_99', 'pred_100']\n",
        "\n",
        "    masks_pred = []\n",
        "    for im in result:\n",
        "        threshold = np.max(im) * threshold_factor\n",
        "        pred = (im > threshold).astype(np.uint8)\n",
        "        pred = binary_dilation(pred, iterations=1)\n",
        "        masks_pred.append(pred)\n",
        "\n",
        "    if format == 'nii':\n",
        "        # save as nifti\n",
        "        for (i, j, k) in zip(masks_pred, reference_files, ids):\n",
        "            img = nib.load(j)\n",
        "            nii = nib.Nifti1Image(i, img.affine, img.header)\n",
        "            nib.save(nii, os.path.join(save_path, f'{k}.nii'))\n",
        "\n",
        "    elif format == 'npy':\n",
        "        # save as numpy\n",
        "        for (m, n) in zip(masks_pred, ids):\n",
        "            np.save(os.path.join(save_path, f'{n}.npy'), m)\n",
        "\n",
        "    return masks_pred"
      ],
      "metadata": {
        "id": "l5ebu20_qFZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics"
      ],
      "metadata": {
        "id": "m1L1HdFiOOj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(y_true, y_pred, mean=True):\n",
        "   Y_true = []\n",
        "    for i in y_true:\n",
        "        true = nib.load(i).get_fdata()\n",
        "        true = binary_dilation(true, iterations=1)\n",
        "        true = tf.cast(true, dtype=tf.float64)\n",
        "        Y_true.append(true)\n",
        "\n",
        "    Y_pred = []\n",
        "    for j in y_pred:\n",
        "        pred = nib.load(j).get_fdata()\n",
        "        pred = binary_dilation(pred, iterations=1)\n",
        "        if mean == True:\n",
        "           pred = np.mean(pred,axis=2)\n",
        "        else:\n",
        "          pred = pred\n",
        "        pred = tf.cast(pred, dtype=tf.float64)\n",
        "        Y_pred.append(pred)\n",
        "\n",
        "    dice = []\n",
        "    for (m,n) in zip(Y_true,Y_pred):\n",
        "      dc = dice_coef(m,n)\n",
        "      dc = np.ravel(dc)\n",
        "      dice.append(dc)\n",
        "\n",
        "    mean_dice = np.mean(dice)\n",
        "    std_dice = np.std(dice)\n",
        "\n",
        "    print(f'The mean dice score is: {mean_dice:.4f} and the standard deviation is: {std_dice:.4f}')\n",
        "    return mean_dice, std_dice"
      ],
      "metadata": {
        "id": "HSaaj0S2shCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3QCw_IRiR2P"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqKV7KDjk0Ku"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDoTqzGPihXj"
      },
      "outputs": [],
      "source": [
        "##Train data ------------------------------------------------------------------\n",
        "peaks_path = 'path/to/data'  #set the path to the train peaks images\n",
        "peaks_images  = sorted(glob.glob(peaks_path_hcp + '*'))\n",
        "mask_path =  'path/to/data'  #set the path to the train binary mask images\n",
        "mask_images  = sorted(glob.glob(mask_path_hcp + '*'))\n",
        "\n",
        "val_peaks_path  = 'path/to/data' #set the path to the validation peaks images\n",
        "val_peaks_images  = sorted(glob.glob(val_peaks_path_hcp + '*'))\n",
        "val_mask_path =  'path/to/data' #set the path to the validation binary mask images\n",
        "val_mask_images  = sorted(glob.glob(val_mask_path_hcp + '*'))\n",
        "\n",
        "#Save weights and predictions path ---------------------------------------------\n",
        "save_model_path_coronal  = 'save/directory/coronal/model' #set the path to save your coronal model weights\n",
        "save_model_path_axial  = 'save/directory/axial/model' #set the path to save your axial model weights\n",
        "save_model_path_sagittal  = 'save/directory/sagittal/model' #set the path to save your sagital model weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7o0txCjiXEz"
      },
      "source": [
        "### Workflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training coronal view\n",
        "print('Training coronal view ...')\n",
        "print('Preprocessing data...')\n",
        "coronal_peaks = np.asarray(prepare_data(peaks_images, 'peaks', 'coronal'))\n",
        "coronal_masks = np.asarray(prepare_data(mask_images, 'masks', 'coronal'))\n",
        "val_coronal_peaks = np.asarray(prepare_data(val_peaks_images, 'peaks', 'coronal'))\n",
        "val_coronal_masks = np.asarray(prepare_data(val_masks_images, 'masks', 'coronal'))\n",
        "print('Train...')\n",
        "coronal_histories = train_unet(coronal_peaks, coronal_masks,\n",
        "                               val_coronal_peaks, val_coronal_masks,\n",
        "                               save_model_path_coronal,\n",
        "                               batch_size=56, epochs=130, learning_rate=0.002,\n",
        "                               shuffle=False, verbose=1)"
      ],
      "metadata": {
        "id": "mLTbxhv4tSjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training axial view\n",
        "print('Training axial view ...')\n",
        "axial_peaks = np.asarray(prepare_data(peaks_images, 'peaks', 'axial'))\n",
        "axial_masks = np.asarray(prepare_data(masks_images, 'masks', 'axial'))\n",
        "val_axial_peaks = np.asarray(prepare_data(val_peaks_images, 'peaks', 'axial'))\n",
        "val_axial_masks = np.asarray(prepare_data(val_masks_images, 'masks', 'axial'))\n",
        "print('Train...')\n",
        "axial_histories = train_unet(axial_peaks, axial_masks,\n",
        "                             val_axial_peaks, val_axial_masks,\n",
        "                             save_model_path_axial,\n",
        "                             batch_size=56, epochs=130, learning_rate=0.002,\n",
        "                             shuffle=False, verbose=1)"
      ],
      "metadata": {
        "id": "dVhP_EV_0eYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training sagittal view\n",
        "print('Training sagittal view ...')\n",
        "sagittal_peaks = np.asarray(prepare_data(peaks_images, 'peaks', 'sagittal'))\n",
        "sagittal_masks = np.asarray(prepare_data(masks_images, 'masks', 'sagittal'))\n",
        "val_sagittal_peaks = np.asarray(prepare_data(val_peaks_images, 'peaks', 'sagittal'))\n",
        "val_sagittal_masks = np.asarray(prepare_data(val_masks_images, 'masks', 'sagittal'))\n",
        "print('Train...')\n",
        "sagittal_histories = train_unet(sagittal_peaks, sagittal_masks,\n",
        "                                val_sagittal_peaks, val_sagittal_masks,\n",
        "                                save_model_path_sagittal,\n",
        "                                batch_size=56, epochs=130, learning_rate=0.002,\n",
        "                                shuffle=False, verbose=1)"
      ],
      "metadata": {
        "id": "4ROFdUMO8rD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jycS7-eT-V_B"
      },
      "source": [
        "##Test and Dice Scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Set Up"
      ],
      "metadata": {
        "id": "fUA-oqH61zoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Test data ------------------------------------------------------------------\n",
        "test_peaks_path = 'path/to/data'  #set the path to the test peaks images\n",
        "test_peaks_images  = sorted(glob.glob(test_peaks_path + '*'))\n",
        "\n",
        "reference_masks_path = 'path/to/data'  #set the path to reference mask images (this is to save the predictions in .nii extension, so you need a reference header)\n",
        "reference_masks_images  = sorted(glob.glob(reference_masks_path + '*'))\n",
        "\n",
        "#Local to weights and save predictions directory ---------------------------------------------\n",
        "model_path_coronal  = 'directory/coronal/model' #set the path to coronal model weights\n",
        "model_path_axial  = 'directory/axial/model' #set the path to axial model weights\n",
        "model_path_sagittal  = 'directory/sagittal/model' #set the path to sagital model weights\n",
        "\n",
        "save_results_path = 'save/directory/predictions/results' #set the path to save your results (predictions)"
      ],
      "metadata": {
        "id": "2Q3cdgRTvTIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test in new data"
      ],
      "metadata": {
        "id": "YQkDEEQP17dU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5GKvkCVjd_F"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "\n",
        "print('Predicting coronal view ...')\n",
        "coronal_predictions = predict_new_data(test_peaks_images, model_path_coronal, 'coronal', 60)\n",
        "\n",
        "print('Predicting axial view ...')\n",
        "axial_predictions = predict_new_data(test_peaks_images, save_model_path_axial, 'axial', 60)\n",
        "\n",
        "print('Predicting sagittal view ...')\n",
        "sagittal_predictions = predict_new_data(test_peaks_images, save_model_path_sagittal, 'sagittal', 60)\n",
        "\n",
        "print('Concatenating the results and saving ...')\n",
        "save_predictions(coronal_predictions, axial_predictions, sagittal_predictions,\n",
        "                 flip=False, mean=True, resize=True,\n",
        "                 reference_files=reference_masks_images, save_path=save_results_path,\n",
        "                 format='nii', threshold_factor=0.5) #the threshold can change depending the tracts that you are interested, so always check the results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dice scores from the predictions"
      ],
      "metadata": {
        "id": "vLabJ8oi2A2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dice Scores\n",
        "y_pred_path = 'path/to/your/results/'\n",
        "y_pred = sorted(glob.glob(y_pred_path + '*'))\n",
        "#Ground truth\n",
        "y_true_path = 'path/to/ground/truth/'\n",
        "y_true = sorted(glob.glob(y_true_path + '*'))\n",
        "\n",
        "dice = dice_score(y_true, y_pred, mean=False)"
      ],
      "metadata": {
        "id": "4lIzNuhSMsQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1a1216-7c69-442a-bf26-27020e51f918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean dice score is: 0.7238 and the standard deviation is: 0.0541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the results\n",
        "ground_truth = nib.load(y_true[01]).get_fdata()\n",
        "ground_truth_slice = ground_truth[75,:,:]\n",
        "\n",
        "pred = nib.load(y_pred[01]).get_fdata()\n",
        "pred_image_slice = pred[75,:,:]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(ground_truth_slic)\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(pred_image_slice)"
      ],
      "metadata": {
        "id": "VtOtIQOx0tH4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Csy-HsXqe289",
        "zYuF7t5we-u9",
        "tpHrwg92CGtE",
        "wl7bGwzQCYn2",
        "q6qDTfYaCfVw",
        "4kZkaqyICo-m",
        "Pvd1mH-DOT6z",
        "m1L1HdFiOOj2",
        "t3QCw_IRiR2P",
        "kqKV7KDjk0Ku",
        "m7o0txCjiXEz",
        "jycS7-eT-V_B",
        "fUA-oqH61zoa",
        "YQkDEEQP17dU",
        "vLabJ8oi2A2-"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}